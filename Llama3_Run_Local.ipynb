{"cells":[{"cell_type":"markdown","id":"aaac179b","metadata":{"id":"aaac179b"},"source":["# Project: Running Llama 3.1 Locally"]},{"cell_type":"markdown","source":["The code below was developed on a Google Colab. It experiments with running Llama 3.1 locally using Ollama.\n","\n","Results: The 8B parameter version could be run on a T4 instance and consumed about 6GB of the 15GB GPU memory that was available and had reasonable response time. The 70B parameter version was able to run on a A100 but it consumed all the 40GB of GPU memory available, indicating available memory was insufficient. The response time for the simple requests below took longer for the 70B model on an A100 instance than the 8B on a T4. This again provides some evidence that the 70B model requires more than 40GB of GPU memory to run optimally.     "],"metadata":{"id":"WSgTNyjtFxX3"},"id":"WSgTNyjtFxX3"},{"cell_type":"markdown","source":["## Install and Run Ollama Server"],"metadata":{"id":"yLtON3tDFLG1"},"id":"yLtON3tDFLG1"},{"cell_type":"markdown","source":[],"metadata":{"id":"P2kapZfpFsjV"},"id":"P2kapZfpFsjV"},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/MyDrive/projects/LLM/AgenticRAG/rag_agents"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EE21R8TR_qi1","executionInfo":{"status":"ok","timestamp":1722881401366,"user_tz":240,"elapsed":23760,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}},"outputId":"5f3cf301-d3dc-466f-8403-811b8218b8fe"},"id":"EE21R8TR_qi1","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/projects/LLM/AgenticRAG/rag_agents\n"]}]},{"cell_type":"code","execution_count":2,"id":"af9206b9","metadata":{"id":"af9206b9","executionInfo":{"status":"ok","timestamp":1722880006051,"user_tz":240,"elapsed":10289,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}}},"outputs":[],"source":["%%capture --no-stderr\n","%pip install -qU langchain langchain_community langchain-openai langchainhub langchain-ollama"]},{"cell_type":"markdown","id":"7951f8d8","metadata":{"id":"7951f8d8"},"source":["Below are the commands to install and run Ollama on Ubuntu."]},{"cell_type":"markdown","source":["Run the comands below if running on AWS Ubuntu."],"metadata":{"id":"Nj5f1_yxBQKN"},"id":"Nj5f1_yxBQKN"},{"cell_type":"code","source":["!sudo apt update && sudo apt upgrade --assume-yes\n","!sudo apt install curl --assume-yes\n","!curl --version"],"metadata":{"id":"oGa_uc9QBPOF"},"id":"oGa_uc9QBPOF","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Install and run Ollama server."],"metadata":{"id":"k5hq23DLBfMF"},"id":"k5hq23DLBfMF"},{"cell_type":"code","execution_count":2,"id":"f9585abc","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f9585abc","executionInfo":{"status":"ok","timestamp":1722880755252,"user_tz":240,"elapsed":5262,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}},"outputId":"4b5e205b-d726-416d-c4df-d0ef5b5fcc1e"},"outputs":[{"output_type":"stream","name":"stdout","text":[">>> Downloading ollama...\n","############################################################################################# 100.0%\n",">>> Installing ollama to /usr/local/bin...\n",">>> Adding ollama user to video group...\n",">>> Adding current user to ollama group...\n",">>> Creating ollama systemd service...\n","WARNING: Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",">>> The Ollama API is now available at 127.0.0.1:11434.\n",">>> Install complete. Run \"ollama\" from the command line.\n"]}],"source":["!curl -fsSL https://ollama.com/install.sh | sh\n","!ollama serve > server.out 2>&1 &"]},{"cell_type":"markdown","source":["## Test Calling Local Llama 3.1"],"metadata":{"id":"6FFGmiCcFTAv"},"id":"6FFGmiCcFTAv"},{"cell_type":"markdown","id":"95b89064","metadata":{"id":"95b89064"},"source":["The code below is used to create Llama 3.1 clients. It takes a model parameter so different model version clients can be created.\n","\n","A LangChain pipeline is used to submit requests."]},{"cell_type":"code","execution_count":3,"id":"8a7d2512-f7be-474d-936f-a24981a355b4","metadata":{"id":"8a7d2512-f7be-474d-936f-a24981a355b4","executionInfo":{"status":"ok","timestamp":1722880767201,"user_tz":240,"elapsed":1960,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}}},"outputs":[],"source":["from langchain_ollama import ChatOllama\n","from langchain.prompts import PromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","\n","def get_llama_client(version:str):\n","  \"\"\"\n","  Create a Llama 3.1 client.\n","  Parametrs:\n","    version (str): The Llama 3.1 model version to use, e.g. llama3.1:8b, llama3.1:70b.\n","  Returns:\n","    chain (object): A Llama 3.1 client.\n","  \"\"\"\n","\n","  prompt = PromptTemplate(\n","      template=\"\"\"You are a conscience Meta Llama 3.1 model.\n","      Use one sentence to answer the question concisely.\n","      Question: {question}\n","      Answer:\n","      \"\"\",\n","      input_variables=[\"question\"],\n","  )\n","  llm = ChatOllama(\n","      model=version,\n","      temperature=0,\n","  )\n","  chain = prompt | llm | StrOutputParser()\n","\n","  return chain"]},{"cell_type":"markdown","source":["## llama3.1:8b\n","\n","Below, the 8B version was run on a Colab T4 instance using approximatgely 6 GB of GPU memory.\n","\n","The command below starts a 8B instance. Even though this command will return instantly, the instance will take about a minute to start. Monitor the \"model.out\" log file to determine when the model is available."],"metadata":{"id":"OCYwXegeSU_O"},"id":"OCYwXegeSU_O"},{"cell_type":"code","source":["!ollama run llama3.1:8b > model.out 2>&1 &"],"metadata":{"id":"iATQug6SLX3h"},"id":"iATQug6SLX3h","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Lets see how much memory is used."],"metadata":{"id":"X0P7L-im_xQS"},"id":"X0P7L-im_xQS"},{"cell_type":"code","execution_count":null,"id":"8704234d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8704234d","executionInfo":{"status":"ok","timestamp":1722701414281,"user_tz":240,"elapsed":210,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}},"outputId":"da7fb96d-5833-476c-b14f-82b585070064"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sat Aug  3 16:10:13 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   61C    P0              30W /  70W |   6097MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"markdown","source":["About 6GB of GPU memory is used.\n","\n","Lets submit some requests."],"metadata":{"id":"8zbMTX1ZAaN8"},"id":"8zbMTX1ZAaN8"},{"cell_type":"code","source":["chain = get_llama_client('llama3.1:8b')"],"metadata":{"id":"xyWzoMR2All0"},"id":"xyWzoMR2All0","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"671c460d-f5bf-4a80-bab6-86453aececa4","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"id":"671c460d-f5bf-4a80-bab6-86453aececa4","executionInfo":{"status":"ok","timestamp":1722701919266,"user_tz":240,"elapsed":1322,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}},"outputId":"a60b8fed-2f2f-47ac-d805-d52bf432a58e"},"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 87.3 ms, sys: 2.06 ms, total: 89.4 ms\n","Wall time: 1.16 s\n"]},{"output_type":"execute_result","data":{"text/plain":["'My knowledge cutoff is December 2023, but I have been trained on a broader range of topics and can provide more up-to-date information in some areas.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}],"source":["%%time\n","chain.invoke({'question':\n","              'What is the knowledge cutoff for Meta Llama 3.1?'})"]},{"cell_type":"code","source":["%%time\n","chain.invoke({'question':\n","              'How much data was used to train Meta Llama 3.1?'})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"id":"2QpPbPHSG8dv","executionInfo":{"status":"ok","timestamp":1722701938479,"user_tz":240,"elapsed":1868,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}},"outputId":"4430df88-8273-43dc-e5d4-1cbc06d43f84"},"id":"2QpPbPHSG8dv","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 110 ms, sys: 3.95 ms, total: 114 ms\n","Wall time: 1.67 s\n"]},{"output_type":"execute_result","data":{"text/plain":["\"The training dataset for Meta Llama 3.1 is not publicly disclosed, but it's reported to be a massive corpus of text data sourced from various places, including but not limited to the internet, books, and user-generated content.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["A Google search shows that the \"December 2023\" response is correct. However, also per Google, ~15 trillion tokens where used to train Llama 3.1. Its possible this information wasn't available at the time of training.\n","\n","Below, the model is removed from the Ollama server."],"metadata":{"id":"9XF82ZJj8shN"},"id":"9XF82ZJj8shN"},{"cell_type":"code","source":["!ollama rm llama3.1:8b"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cxEor6BGI174","executionInfo":{"status":"ok","timestamp":1722702249574,"user_tz":240,"elapsed":684,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}},"outputId":"a671adfe-714a-4bd7-ea14-becb129e4830"},"id":"cxEor6BGI174","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["deleted 'llama3.1:8b'\n"]}]},{"cell_type":"markdown","source":["## llama3:8b-instruct-q2_K"],"metadata":{"id":"Ppf4e1MEk7l4"},"id":"Ppf4e1MEk7l4"},{"cell_type":"markdown","source":["Lets experiment with the smallest quantized version."],"metadata":{"id":"U5SGhEVslA3h"},"id":"U5SGhEVslA3h"},{"cell_type":"code","source":["!ollama run llama3:8b-instruct-q2_K > model.out 2>&1 &"],"metadata":{"id":"fMdLeLrflLAZ","executionInfo":{"status":"ok","timestamp":1722879056801,"user_tz":240,"elapsed":682,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}}},"id":"fMdLeLrflLAZ","execution_count":5,"outputs":[]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LLwWTNQSlQqV","executionInfo":{"status":"ok","timestamp":1722879124113,"user_tz":240,"elapsed":273,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}},"outputId":"11bf098d-9069-4d8d-aae2-4801f327d216"},"id":"LLwWTNQSlQqV","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Mon Aug  5 17:32:03 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   48C    P0              28W /  70W |   4801MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","+---------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"markdown","source":["As expected, the GPU memory used is much smaller than the full-floating point version.\n","\n","Requests are submitted below."],"metadata":{"id":"L90J31yUmX7J"},"id":"L90J31yUmX7J"},{"cell_type":"code","source":["chain = get_llama_client('llama3:8b-instruct-q2_K')"],"metadata":{"id":"-lepdEatlZYx","executionInfo":{"status":"ok","timestamp":1722879194067,"user_tz":240,"elapsed":490,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}}},"id":"-lepdEatlZYx","execution_count":7,"outputs":[]},{"cell_type":"code","source":["%%time\n","chain.invoke({'question':\n","              'What is the knowledge cutoff for Meta Llama 3.1?'})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"id":"R2kRCkV5ljqg","executionInfo":{"status":"ok","timestamp":1722879219472,"user_tz":240,"elapsed":1958,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}},"outputId":"ee2afb82-168c-41fa-878d-1a4cd7718e69"},"id":"R2kRCkV5ljqg","execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 86.8 ms, sys: 0 ns, total: 86.8 ms\n","Wall time: 1.41 s\n"]},{"output_type":"execute_result","data":{"text/plain":["'The knowledge cutoff for Meta Llama 3.1 is approximately the end of 2022, with a focus on its training data being up to October 2021.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["%%time\n","chain.invoke({'question':\n","              'How much data was used to train Meta Llama 3.1?'})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"id":"NBJmH2rulmL3","executionInfo":{"status":"ok","timestamp":1722879227676,"user_tz":240,"elapsed":1378,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}},"outputId":"021b65d1-f878-4b28-9622-ee1ae5f830d7"},"id":"NBJmH2rulmL3","execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 75.2 ms, sys: 0 ns, total: 75.2 ms\n","Wall time: 1.1 s\n"]},{"output_type":"execute_result","data":{"text/plain":["'The Meta Llama 3.1 model was trained on approximately 128 million parameters and 13 billion tokens of text data.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["While the GPU memory usages is smaller than full 8B version, the response time is a out the same for these simple requests. Furhtermore, both responses are incorrect."],"metadata":{"id":"8_D-Di0PmvZt"},"id":"8_D-Di0PmvZt"},{"cell_type":"code","source":["!ollama rm llama3:8b-instruct-q2_K"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DNnxEr_mn00i","executionInfo":{"status":"ok","timestamp":1722879542249,"user_tz":240,"elapsed":921,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}},"outputId":"519e54b8-ef94-4409-d156-15b5aed80b19"},"id":"DNnxEr_mn00i","execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["deleted 'llama3:8b-instruct-q2_K'\n"]}]},{"cell_type":"markdown","source":["## llama3.1:70b\n","\n","Only a A100 instance with 40GB of GPU memory was able to start and process requests with any kind of reasonable response time. The model used all 40 GB of GPU memory.\n","\n","Below the model is started. Monitor the log file to determine its availability. Complete startup will take a about 8 minutes."],"metadata":{"id":"5To_LnnNR-2N"},"id":"5To_LnnNR-2N"},{"cell_type":"code","source":["!ollama run llama3.1:70b > model.out 2>&1 &"],"metadata":{"id":"JdcWtVsTIoG4","executionInfo":{"status":"ok","timestamp":1722880034567,"user_tz":240,"elapsed":1033,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}}},"id":"JdcWtVsTIoG4","execution_count":5,"outputs":[]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tbjF_0Z2pTTn","executionInfo":{"status":"ok","timestamp":1722880418585,"user_tz":240,"elapsed":509,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}},"outputId":"283eea25-7266-472c-8a59-2639f4b932fd"},"id":"tbjF_0Z2pTTn","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Mon Aug  5 17:53:37 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   33C    P0              50W / 400W |  39261MiB / 40960MiB |      0%      Default |\n","|                                         |                      |             Disabled |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","+---------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"markdown","source":["Just about all GPU memory is being used. Likely meaning more is requried for this version to run effectively."],"metadata":{"id":"5_7IExGVrSvz"},"id":"5_7IExGVrSvz"},{"cell_type":"code","source":["chain = get_llama_client('llama3.1:70b')"],"metadata":{"id":"1HVr4NUJEo0_","executionInfo":{"status":"ok","timestamp":1722880477782,"user_tz":240,"elapsed":1169,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}}},"id":"1HVr4NUJEo0_","execution_count":7,"outputs":[]},{"cell_type":"code","source":["%%time\n","chain.invoke({'question': 'What is the knowledge cutoff for Meta Llama 3.1?'})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"id":"aF4WHgwcJNqt","executionInfo":{"status":"ok","timestamp":1722880484967,"user_tz":240,"elapsed":3038,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}},"outputId":"734a76e4-0ece-4252-8ac9-0c6a8c4e769c"},"id":"aF4WHgwcJNqt","execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 93.3 ms, sys: 6.27 ms, total: 99.6 ms\n","Wall time: 2.59 s\n"]},{"output_type":"execute_result","data":{"text/plain":["'The knowledge cutoff for Meta Llama 3.1 is December 2022, meaning it was trained on data available up to that point in time.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["%%time\n","chain.invoke({'question':\n","              'How much data was used to train Meta Llama 3.1?'})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"id":"4EPKMcFuR0at","executionInfo":{"status":"ok","timestamp":1722880493114,"user_tz":240,"elapsed":2948,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}},"outputId":"bf8b92fd-7cb6-47e8-a599-2d33019dbd8c"},"id":"4EPKMcFuR0at","execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 87.7 ms, sys: 6.64 ms, total: 94.4 ms\n","Wall time: 2.25 s\n"]},{"output_type":"execute_result","data":{"text/plain":["'Meta Llama 3.1 was trained on approximately 2 trillion parameters and 1.5 billion tokens of text data.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["The answers are different from the 8B version and, as far as I can tell, incorrect. The response time is almost double the 8B version, as well."],"metadata":{"id":"E8fgDsX8Ilpe"},"id":"E8fgDsX8Ilpe"},{"cell_type":"code","source":["!ollama rm llama3.1:70b"],"metadata":{"id":"z_3scoOY9MvY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722880510691,"user_tz":240,"elapsed":380,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}},"outputId":"cbcb572f-0b70-4d76-8180-66dae11fccd8"},"id":"z_3scoOY9MvY","execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["deleted 'llama3.1:70b'\n"]}]},{"cell_type":"markdown","source":["## llama3:70b-instruct-q2_K"],"metadata":{"id":"-J-dKOYbomQA"},"id":"-J-dKOYbomQA"},{"cell_type":"code","source":["!ollama run llama3:70b-instruct-q2_K > model.out 2>&1 &"],"metadata":{"id":"htNPBgdoonCd","executionInfo":{"status":"ok","timestamp":1722880776402,"user_tz":240,"elapsed":1027,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}}},"id":"htNPBgdoonCd","execution_count":4,"outputs":[]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2xOJwvO7sBMa","executionInfo":{"status":"ok","timestamp":1722881136055,"user_tz":240,"elapsed":433,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}},"outputId":"ef31ec89-1727-4586-88c2-b6c3e54d0606"},"id":"2xOJwvO7sBMa","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mon Aug  5 18:05:35 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   33C    P0              50W / 400W |  29289MiB / 40960MiB |      0%      Default |\n","|                                         |                      |             Disabled |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","+---------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["chain = get_llama_client('llama3:70b-instruct-q2_K')"],"metadata":{"id":"HmFwX6xfpBkU","executionInfo":{"status":"ok","timestamp":1722881150981,"user_tz":240,"elapsed":507,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}}},"id":"HmFwX6xfpBkU","execution_count":6,"outputs":[]},{"cell_type":"code","source":["%%time\n","chain.invoke({'question': 'What is the knowledge cutoff for Meta Llama 3.1?'})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":72},"id":"lOMEy77HpIOi","executionInfo":{"status":"ok","timestamp":1722881156770,"user_tz":240,"elapsed":2409,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}},"outputId":"f78de5e3-a8a5-4782-882b-f1596677f8c9"},"id":"lOMEy77HpIOi","execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 82.1 ms, sys: 2.42 ms, total: 84.6 ms\n","Wall time: 1.32 s\n"]},{"output_type":"execute_result","data":{"text/plain":["'The knowledge cutoff for Meta Llama 3.1 is December 2022.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["%%time\n","chain.invoke({'question':\n","              'How much data was used to train Meta Llama 3.1?'})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":108},"id":"zVTC-tYTpKTl","executionInfo":{"status":"ok","timestamp":1722881180616,"user_tz":240,"elapsed":3013,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}},"outputId":"e4f75076-133e-43bd-94f5-9fb55a60f31d"},"id":"zVTC-tYTpKTl","execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 81.9 ms, sys: 1.31 ms, total: 83.2 ms\n","Wall time: 2.45 s\n"]},{"output_type":"execute_result","data":{"text/plain":["'Meta Llama 3.1 was trained on a massive dataset of approximately 2.5 billion parameters and 20TB of text data, sourced from various websites, books, and other digital platforms.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["!ollama rm llama3:70b-instruct-q2_K"],"metadata":{"id":"tQe4_A7HpKQQ"},"id":"tQe4_A7HpKQQ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Less memory was used compared to the full 70B version. The responses and execution time are similar."],"metadata":{"id":"okzQyEbsuYtG"},"id":"okzQyEbsuYtG"}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"colab":{"provenance":[],"gpuType":"T4","machine_shape":"hm"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}