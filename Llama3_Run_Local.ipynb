{"cells":[{"cell_type":"markdown","id":"aaac179b","metadata":{"id":"aaac179b"},"source":["# Project: Running Llama 3.1 Locally"]},{"cell_type":"markdown","source":["The code below was developed on a Google Colab. It experiments with running Llama 3.1 locally using Ollama.\n","\n","Results: The 8B parameter version could be run on a T4 instance and consumed about 6GB of the 15GB GPU memory that was available and had reasonable response time. The 70B parameter version was able to run on a A100 but it consumed all the 40GB of GPU memory available, indicating available memory was insufficient. The response time for the simple requests below took longer for the 70B model on an A100 instance than the 8B on a T4. This again provides some evidence that the 70B model requires more than 40GB of GPU memory to run optimally.     "],"metadata":{"id":"WSgTNyjtFxX3"},"id":"WSgTNyjtFxX3"},{"cell_type":"markdown","source":["## Install and Run Ollama Server"],"metadata":{"id":"yLtON3tDFLG1"},"id":"yLtON3tDFLG1"},{"cell_type":"markdown","source":[],"metadata":{"id":"P2kapZfpFsjV"},"id":"P2kapZfpFsjV"},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/MyDrive/projects/LLM/AgenticRAG/rag_agents"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EE21R8TR_qi1","executionInfo":{"status":"ok","timestamp":1722702811206,"user_tz":240,"elapsed":117362,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}},"outputId":"e16985fa-a1b5-41ca-91f7-8690169002f6"},"id":"EE21R8TR_qi1","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/projects/LLM/AgenticRAG/rag_agents\n"]}]},{"cell_type":"code","execution_count":2,"id":"af9206b9","metadata":{"id":"af9206b9","executionInfo":{"status":"ok","timestamp":1722702827994,"user_tz":240,"elapsed":12522,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}}},"outputs":[],"source":["%%capture --no-stderr\n","%pip install -qU langchain langchain_community langchain-openai langchainhub langchain-ollama"]},{"cell_type":"markdown","id":"7951f8d8","metadata":{"id":"7951f8d8"},"source":["Below are the commands to install and run Ollama on Ubuntu."]},{"cell_type":"markdown","source":["Run the comands below if running on AWS Ubuntu."],"metadata":{"id":"Nj5f1_yxBQKN"},"id":"Nj5f1_yxBQKN"},{"cell_type":"code","source":["!sudo apt update && sudo apt upgrade --assume-yes\n","!sudo apt install curl --assume-yes\n","!curl --version"],"metadata":{"id":"oGa_uc9QBPOF"},"id":"oGa_uc9QBPOF","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Install and run Ollama server."],"metadata":{"id":"k5hq23DLBfMF"},"id":"k5hq23DLBfMF"},{"cell_type":"code","execution_count":3,"id":"f9585abc","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f9585abc","executionInfo":{"status":"ok","timestamp":1722702838068,"user_tz":240,"elapsed":7437,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}},"outputId":"57c173c2-5de0-4219-962d-c1de84a90e2f"},"outputs":[{"output_type":"stream","name":"stdout","text":[">>> Downloading ollama...\n","############################################################################################# 100.0%\n",">>> Installing ollama to /usr/local/bin...\n",">>> Creating ollama user...\n",">>> Adding ollama user to video group...\n",">>> Adding current user to ollama group...\n",">>> Creating ollama systemd service...\n","WARNING: Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",">>> The Ollama API is now available at 127.0.0.1:11434.\n",">>> Install complete. Run \"ollama\" from the command line.\n"]}],"source":["!curl -fsSL https://ollama.com/install.sh | sh\n","!ollama serve > server.out 2>&1 &"]},{"cell_type":"markdown","source":["## Test Calling Local Llama 3.1"],"metadata":{"id":"6FFGmiCcFTAv"},"id":"6FFGmiCcFTAv"},{"cell_type":"markdown","id":"95b89064","metadata":{"id":"95b89064"},"source":["The code below is used to create Llama 3.1 clients. It takes a model parameter so different model version clients can be created.\n","\n","A LangChain pipeline is used to submit requests."]},{"cell_type":"code","execution_count":4,"id":"8a7d2512-f7be-474d-936f-a24981a355b4","metadata":{"id":"8a7d2512-f7be-474d-936f-a24981a355b4","executionInfo":{"status":"ok","timestamp":1722702865494,"user_tz":240,"elapsed":932,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}}},"outputs":[],"source":["from langchain_ollama import ChatOllama\n","from langchain.prompts import PromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","\n","def get_llama_client(version:str):\n","  \"\"\"\n","  Create a Llama 3.1 client.\n","  Parametrs:\n","    version (str): The Llama 3.1 model version to use, e.g. llama3.1:8b, llama3.1:70b.\n","  Returns:\n","    chain (object): A Llama 3.1 client.\n","  \"\"\"\n","\n","  prompt = PromptTemplate(\n","      template=\"\"\"You are a conscience Meta Llama 3.1 model.\n","      Use one sentence to answer the question concisely.\n","      Question: {question}\n","      Answer:\n","      \"\"\",\n","      input_variables=[\"question\"],\n","  )\n","  llm = ChatOllama(\n","      model=version,\n","      temperature=0,\n","  )\n","  chain = prompt | llm | StrOutputParser()\n","\n","  return chain"]},{"cell_type":"markdown","source":["## llama3.1:8b\n","\n","Below, the 8B version was run on a Colab T4 instance using approximatgely 6 GB of GPU memory.\n","\n","The command below starts a 8B instance. Even though this command will return instantly, the instance will take about a minute to start. Monitor the \"model.out\" log file to determine when the model is available."],"metadata":{"id":"OCYwXegeSU_O"},"id":"OCYwXegeSU_O"},{"cell_type":"code","source":["!ollama run llama3.1:8b > model.out 2>&1 &"],"metadata":{"id":"iATQug6SLX3h","executionInfo":{"status":"ok","timestamp":1722701109069,"user_tz":240,"elapsed":615,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}}},"id":"iATQug6SLX3h","execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["Lets see how much memory is used."],"metadata":{"id":"X0P7L-im_xQS"},"id":"X0P7L-im_xQS"},{"cell_type":"code","execution_count":7,"id":"8704234d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8704234d","executionInfo":{"status":"ok","timestamp":1722701414281,"user_tz":240,"elapsed":210,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}},"outputId":"da7fb96d-5833-476c-b14f-82b585070064"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sat Aug  3 16:10:13 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   61C    P0              30W /  70W |   6097MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"markdown","source":["About 6GB of GPU memory is used.\n","\n","Lets submit some requests."],"metadata":{"id":"8zbMTX1ZAaN8"},"id":"8zbMTX1ZAaN8"},{"cell_type":"code","source":["chain = get_llama_client('llama3.1:8b')"],"metadata":{"id":"xyWzoMR2All0","executionInfo":{"status":"ok","timestamp":1722701477339,"user_tz":240,"elapsed":197,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}}},"id":"xyWzoMR2All0","execution_count":8,"outputs":[]},{"cell_type":"code","execution_count":16,"id":"671c460d-f5bf-4a80-bab6-86453aececa4","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"id":"671c460d-f5bf-4a80-bab6-86453aececa4","executionInfo":{"status":"ok","timestamp":1722701919266,"user_tz":240,"elapsed":1322,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}},"outputId":"a60b8fed-2f2f-47ac-d805-d52bf432a58e"},"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 87.3 ms, sys: 2.06 ms, total: 89.4 ms\n","Wall time: 1.16 s\n"]},{"output_type":"execute_result","data":{"text/plain":["'My knowledge cutoff is December 2023, but I have been trained on a broader range of topics and can provide more up-to-date information in some areas.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}],"source":["%%time\n","chain.invoke({'question':\n","              'What is the knowledge cutoff for Meta Llama 3.1?'})"]},{"cell_type":"code","source":["%%time\n","chain.invoke({'question':\n","              'How much data was used to train Meta Llama 3.1?'})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"id":"2QpPbPHSG8dv","executionInfo":{"status":"ok","timestamp":1722701938479,"user_tz":240,"elapsed":1868,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}},"outputId":"4430df88-8273-43dc-e5d4-1cbc06d43f84"},"id":"2QpPbPHSG8dv","execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 110 ms, sys: 3.95 ms, total: 114 ms\n","Wall time: 1.67 s\n"]},{"output_type":"execute_result","data":{"text/plain":["\"The training dataset for Meta Llama 3.1 is not publicly disclosed, but it's reported to be a massive corpus of text data sourced from various places, including but not limited to the internet, books, and user-generated content.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["A Google search shows that the \"December 2023\" response is correct. However, also per Google, ~15 trillion tokens where used to train Llama 3.1. Its possible this information wasn't available at the time of training.\n","\n","Below, the model is removed from the Ollama server."],"metadata":{"id":"9XF82ZJj8shN"},"id":"9XF82ZJj8shN"},{"cell_type":"code","source":["!ollama rm llama3.1:8b"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cxEor6BGI174","executionInfo":{"status":"ok","timestamp":1722702249574,"user_tz":240,"elapsed":684,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}},"outputId":"a671adfe-714a-4bd7-ea14-becb129e4830"},"id":"cxEor6BGI174","execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["deleted 'llama3.1:8b'\n"]}]},{"cell_type":"markdown","source":["## llama3.1:70b\n","\n","Only a A100 instance with 40GB of GPU memory was able to start and process requests with any kind of reasonable response time. The model used all 40 GB of GPU memory.\n","\n","Below the model is started. Monitor the log file to determine its availability. Startup will take a about 8 minutes."],"metadata":{"id":"5To_LnnNR-2N"},"id":"5To_LnnNR-2N"},{"cell_type":"code","source":["!ollama run llama3.1:70b > model.out 2>&1 &"],"metadata":{"id":"JdcWtVsTIoG4","executionInfo":{"status":"ok","timestamp":1722702960660,"user_tz":240,"elapsed":414,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}}},"id":"JdcWtVsTIoG4","execution_count":5,"outputs":[]},{"cell_type":"code","source":["chain = get_llama_client('llama3.1:70b')"],"metadata":{"id":"1HVr4NUJEo0_","executionInfo":{"status":"ok","timestamp":1722703367747,"user_tz":240,"elapsed":156,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}}},"id":"1HVr4NUJEo0_","execution_count":6,"outputs":[]},{"cell_type":"code","source":["%%time\n","chain.invoke({'question': 'What is the knowledge cutoff for Meta Llama 3.1?'})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"aF4WHgwcJNqt","executionInfo":{"status":"ok","timestamp":1722703446279,"user_tz":240,"elapsed":2599,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}},"outputId":"b4fdded0-ad42-4383-e56c-bd4f70e34f08"},"id":"aF4WHgwcJNqt","execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 93.4 ms, sys: 2.54 ms, total: 95.9 ms\n","Wall time: 2.46 s\n"]},{"output_type":"execute_result","data":{"text/plain":["'The knowledge cutoff for Meta Llama 3.1 is December 2022, meaning that my training data only goes up until that point in time.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["%%time\n","chain.invoke({'question':\n","              'How much data was used to train Meta Llama 3.1?'})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"4EPKMcFuR0at","executionInfo":{"status":"ok","timestamp":1722703472157,"user_tz":240,"elapsed":2356,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}},"outputId":"246ff549-02f1-4792-e19f-d29caad4b1c9"},"id":"4EPKMcFuR0at","execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 79.8 ms, sys: 2.18 ms, total: 81.9 ms\n","Wall time: 2.22 s\n"]},{"output_type":"execute_result","data":{"text/plain":["'Meta Llama 3.1 was trained on approximately 2 trillion parameters and 1.5 billion tokens of text data.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["The answers are different from the 8B version and, as far as I can tell, incorrect. The response time is almost double the 8B version, as well."],"metadata":{"id":"E8fgDsX8Ilpe"},"id":"E8fgDsX8Ilpe"},{"cell_type":"code","source":["!ollama rm llama3.1:70b"],"metadata":{"id":"z_3scoOY9MvY"},"id":"z_3scoOY9MvY","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"colab":{"provenance":[],"gpuType":"A100","machine_shape":"hm"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}