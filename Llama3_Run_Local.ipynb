{"cells":[{"cell_type":"markdown","id":"aaac179b","metadata":{"id":"aaac179b"},"source":["# Project: Running Llama 3.1 Locally"]},{"cell_type":"markdown","source":["The code below was developed on a Goolge Colab T4 instance."],"metadata":{"id":"WSgTNyjtFxX3"},"id":"WSgTNyjtFxX3"},{"cell_type":"markdown","source":["## Install and Run Llama 3.1"],"metadata":{"id":"yLtON3tDFLG1"},"id":"yLtON3tDFLG1"},{"cell_type":"markdown","source":[],"metadata":{"id":"P2kapZfpFsjV"},"id":"P2kapZfpFsjV"},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/MyDrive/projects/LLM/AgenticRAG/rag_agents"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EE21R8TR_qi1","executionInfo":{"status":"ok","timestamp":1722639534343,"user_tz":240,"elapsed":18389,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}},"outputId":"4c3bb4a7-1eca-409e-d4a8-3f0174689194"},"id":"EE21R8TR_qi1","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/projects/LLM/AgenticRAG/rag_agents\n"]}]},{"cell_type":"code","execution_count":2,"id":"af9206b9","metadata":{"id":"af9206b9","executionInfo":{"status":"ok","timestamp":1722638355261,"user_tz":240,"elapsed":14258,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}}},"outputs":[],"source":["%%capture --no-stderr\n","%pip install -qU langchain langchain_community langchain-openai langchainhub langchain-ollama"]},{"cell_type":"markdown","id":"7951f8d8","metadata":{"id":"7951f8d8"},"source":["Below are the commands to install and run Ollama on Ubuntu."]},{"cell_type":"markdown","source":["Run the comands below if running on AWS Ubuntu."],"metadata":{"id":"Nj5f1_yxBQKN"},"id":"Nj5f1_yxBQKN"},{"cell_type":"code","source":["!sudo apt update && sudo apt upgrade --assume-yes\n","!sudo apt install curl --assume-yes\n","!curl --version"],"metadata":{"id":"oGa_uc9QBPOF"},"id":"oGa_uc9QBPOF","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Install and run Ollama 3.1 8B."],"metadata":{"id":"k5hq23DLBfMF"},"id":"k5hq23DLBfMF"},{"cell_type":"code","execution_count":4,"id":"f9585abc","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f9585abc","executionInfo":{"status":"ok","timestamp":1722638421133,"user_tz":240,"elapsed":7538,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}},"outputId":"6ae98de7-35cb-42e0-8452-a84eb5751f9b"},"outputs":[{"output_type":"stream","name":"stdout","text":[">>> Downloading ollama...\n","############################################################################################# 100.0%\n",">>> Installing ollama to /usr/local/bin...\n",">>> Creating ollama user...\n",">>> Adding ollama user to video group...\n",">>> Adding current user to ollama group...\n",">>> Creating ollama systemd service...\n","WARNING: Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",">>> The Ollama API is now available at 127.0.0.1:11434.\n",">>> Install complete. Run \"ollama\" from the command line.\n"]}],"source":["!curl -fsSL https://ollama.com/install.sh | sh\n","!ollama serve > server.out 2>&1 &"]},{"cell_type":"code","source":["!ollama run llama3.1:8b > model.out 2>&1 &"],"metadata":{"id":"iATQug6SLX3h"},"id":"iATQug6SLX3h","execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ollama run llama3.1:70b > model.out 2>&1 &"],"metadata":{"id":"Ujk4Q2OUH1KY","executionInfo":{"status":"ok","timestamp":1722638425823,"user_tz":240,"elapsed":583,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}}},"id":"Ujk4Q2OUH1KY","execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":5,"id":"8704234d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8704234d","executionInfo":{"status":"ok","timestamp":1722637761035,"user_tz":240,"elapsed":1070,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}},"outputId":"775d1617-c441-46db-b2bf-1851f0848d1a"},"outputs":[{"output_type":"stream","name":"stdout","text":["               total        used        free      shared  buff/cache   available\n","Mem:            52Gi       1.6Gi        34Gi       2.0Mi        16Gi        50Gi\n","Swap:             0B          0B          0B\n"]}],"source":["!free -mh"]},{"cell_type":"markdown","source":["## Test Calling Local Llama 3.1"],"metadata":{"id":"6FFGmiCcFTAv"},"id":"6FFGmiCcFTAv"},{"cell_type":"markdown","id":"95b89064","metadata":{"id":"95b89064"},"source":["Below the client is created.\n","\n","- llama3.1:8b -> Requires 6 GB of GPU memory.\n","- llama3.1:70b -> Requres 25 GB of GPU memory."]},{"cell_type":"code","execution_count":7,"id":"8a7d2512-f7be-474d-936f-a24981a355b4","metadata":{"id":"8a7d2512-f7be-474d-936f-a24981a355b4","executionInfo":{"status":"ok","timestamp":1722638700765,"user_tz":240,"elapsed":1202,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}}},"outputs":[],"source":["from langchain_ollama import ChatOllama\n","from langchain.prompts import PromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","\n","def get_llama_client(version:str):\n","\n","  prompt = PromptTemplate(\n","      template=\"\"\"You are a conscience Meta Llama 3.1 model.\n","      Use one sentence to answer the question concisely.\n","      Question: {question}\n","      Answer:\n","      \"\"\",\n","      input_variables=[\"question\"],\n","  )\n","  llm = ChatOllama(\n","      model=version,\n","      temperature=0,\n","  )\n","  chain = prompt | llm | StrOutputParser()\n","\n","  return chain"]},{"cell_type":"markdown","id":"74fafd3a","metadata":{"id":"74fafd3a"},"source":["Lets test it with a question it shouldn't have information about."]},{"cell_type":"markdown","source":["## llama3.1:8b\n","\n","6 GB of GPU memory"],"metadata":{"id":"OCYwXegeSU_O"},"id":"OCYwXegeSU_O"},{"cell_type":"code","execution_count":39,"id":"671c460d-f5bf-4a80-bab6-86453aececa4","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"id":"671c460d-f5bf-4a80-bab6-86453aececa4","executionInfo":{"status":"ok","timestamp":1722636098164,"user_tz":240,"elapsed":1259,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}},"outputId":"1e31d48d-f239-4ecf-f325-67586aea1260"},"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 108 ms, sys: 507 Âµs, total: 109 ms\n","Wall time: 1.07 s\n"]},{"output_type":"execute_result","data":{"text/plain":["'My knowledge cutoff is December 2023, but I can provide information and insights based on my training data up until that point.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":39}],"source":["%%time\n","chain = get_llama_client('llama3.1:8b')\n","chain.invoke({'question':\n","              'What is your knowledge cutoff?'})"]},{"cell_type":"code","source":["%%time\n","chain.invoke({'question':\n","              'How much data was used to train you?'})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"id":"2QpPbPHSG8dv","executionInfo":{"status":"ok","timestamp":1722636103776,"user_tz":240,"elapsed":1375,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}},"outputId":"cb7038d9-94bd-4ec3-c5aa-453b1eb4705c"},"id":"2QpPbPHSG8dv","execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 104 ms, sys: 3.47 ms, total: 107 ms\n","Wall time: 1.25 s\n"]},{"output_type":"execute_result","data":{"text/plain":["'I was trained on a massive dataset of over 52 terabytes, which is roughly equivalent to the contents of 10 million books or 1.5 billion web pages.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["!ollama rm llama3.1:8b"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cxEor6BGI174","executionInfo":{"status":"ok","timestamp":1722636568480,"user_tz":240,"elapsed":180,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}},"outputId":"691e8df0-eeac-4d91-dbaf-919a479d8258"},"id":"cxEor6BGI174","execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["Error: open /root/.ollama/models/manifests/registry.ollama.ai/library/llama3.1/8b: no such file or directory\n"]}]},{"cell_type":"markdown","source":["## llama3.1:70b\n","\n","Maxed out all 40 GB of GPU memory on A100."],"metadata":{"id":"5To_LnnNR-2N"},"id":"5To_LnnNR-2N"},{"cell_type":"code","source":["!ollama run llama3.1:70b > model.out 2>&1 &"],"metadata":{"id":"JdcWtVsTIoG4","executionInfo":{"status":"ok","timestamp":1722636860204,"user_tz":240,"elapsed":218,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}}},"id":"JdcWtVsTIoG4","execution_count":45,"outputs":[]},{"cell_type":"code","source":["%%time\n","chain = get_llama_client('llama3.1:70b')\n","chain.invoke({'question': 'What is your knowledge cutoff?'})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"id":"aF4WHgwcJNqt","executionInfo":{"status":"ok","timestamp":1722638873263,"user_tz":240,"elapsed":3477,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}},"outputId":"eca98f67-d6aa-4ccc-b961-09ac40388c75"},"id":"aF4WHgwcJNqt","execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 99.4 ms, sys: 6.54 ms, total: 106 ms\n","Wall time: 3.32 s\n"]},{"output_type":"execute_result","data":{"text/plain":["'My knowledge cutoff is currently December 2022, which means I have been trained on data up to that point and may not be aware of events or developments that have occurred after that date.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["%%time\n","chain.invoke({'question':\n","              'How much data was used to train you?'})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"id":"4EPKMcFuR0at","executionInfo":{"status":"ok","timestamp":1722638893690,"user_tz":240,"elapsed":3121,"user":{"displayName":"Rick Martel","userId":"08304577219807309529"}},"outputId":"03258731-c83c-4c61-8d78-d9dec8c60cd0"},"id":"4EPKMcFuR0at","execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 87.9 ms, sys: 5.62 ms, total: 93.5 ms\n","Wall time: 2.94 s\n"]},{"output_type":"execute_result","data":{"text/plain":["'I was trained on approximately 2 trillion parameters and 45 terabytes of text data, sourced from a diverse range of sources including books, articles, research papers, and websites.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}